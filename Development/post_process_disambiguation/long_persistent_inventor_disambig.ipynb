{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T20:26:08.646299Z",
     "start_time": "2020-01-27T20:26:08.637343Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from warnings import filterwarnings\n",
    "import csv\n",
    "import re, os, random, string, codecs\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "import time\n",
    "import tqdm\n",
    "project_home = os.environ['PACKAGE_HOME']\n",
    "from Development.helpers import general_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T20:26:09.494014Z",
     "start_time": "2020-01-27T20:26:09.485227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pipeline_datadir/upload_20191231/disambig_output\n",
      "patent_20191008 | patent_20191231 | 20191231\n"
     ]
    }
   ],
   "source": [
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(project_home + '/Development/config.ini')\n",
    "\n",
    "db_con = general_helpers.connect_to_db(config['DATABASE']['HOST'], config['DATABASE']['USERNAME'],\n",
    "                                       config['DATABASE']['PASSWORD'], config['DATABASE']['NEW_DB'])\n",
    "\n",
    "db_folder = 'upload_20191231'\n",
    "disambig_folder = \"{}{}/disambig_output\".format(config['FOLDERS']['WORKING_FOLDER'], db_folder)\n",
    "\n",
    "\n",
    "print(disambig_folder)\n",
    "old_db = config['DATABASE']['OLD_DB']\n",
    "new_db = config['DATABASE']['NEW_DB']\n",
    "new_db_timestamp = new_db.replace('patent_', '')\n",
    "print(old_db, \"|\", new_db, \"|\" , new_db_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T20:32:37.344212Z",
     "start_time": "2020-01-27T20:32:37.304058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disamb_inventor_id_20171226', 'disamb_inventor_id_20171003', 'disamb_inventor_id_20170808', 'disamb_inventor_id_20180528', 'disamb_inventor_id_20181127', 'disamb_inventor_id_20190312', 'disamb_inventor_id_20190820', 'disamb_inventor_id_20191008', 'disamb_inventor_id_20191231']\n",
      "######################################################################\n",
      "\n",
      "['current_rawinventor_id', 'old_rawinventor_id', 'disamb_inventor_id_20171226', 'disamb_inventor_id_20171003', 'disamb_inventor_id_20170808', 'disamb_inventor_id_20180528', 'disamb_inventor_id_20181127', 'disamb_inventor_id_20190312', 'disamb_inventor_id_20190820', 'disamb_inventor_id_20191008', 'disamb_inventor_id_20191231']\n"
     ]
    }
   ],
   "source": [
    "# 1. create output file for wide format\n",
    "\n",
    "# get disambig cols needed\n",
    "result = db_con.execute(\"select column_name from information_schema.columns where table_schema = '{0}' and table_name = '{1}'\".format(new_db, 'persistent_inventor_disambig'))\n",
    "pid_cols = [r[0] for r in result]\n",
    "disambig_cols = [x for x in pid_cols if x.startswith('disamb')]\n",
    "print(disambig_cols)\n",
    "print('######################################################################\\n')\n",
    "\n",
    "outfile_name = disambig_folder +'/persistent_inventor_wide.tsv'\n",
    "\n",
    "# TODO: add back in the + part below for final script - ignoring for now since persistent_inventor_disambig has that column\n",
    "header = ['current_rawinventor_id', 'old_rawinventor_id'] + disambig_cols #+ ['disamb_inventor_id' + new_db_timestamp]\n",
    "print(header)\n",
    "header_df = pd.DataFrame(columns = header)\n",
    "header_df.to_csv(outfile_name, index=False, header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T16:37:37.444085Z",
     "start_time": "2020-01-27T16:37:37.433880Z"
    }
   },
   "outputs": [],
   "source": [
    "outfile_name_newinventors = disambig_folder +'/persistent_inventor_long_20191231.tsv'\n",
    "header = ['uuid', 'database_update', 'inventor_id']\n",
    "header_df = pd.DataFrame(columns = header)\n",
    "header_df.to_csv(outfile_name, index=False, header=True, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T13:21:20.679054Z",
     "start_time": "2020-01-27T13:20:02.745926Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17428664"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get total rows in the rawinventor table\n",
    "result = db_con.execute('select count(*) from {}.{}'.format(new_db,'rawinventor'))\n",
    "total_rows = [r[0] for r in result][0]\n",
    "total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T16:40:34.323995Z",
     "start_time": "2020-01-27T16:40:34.308964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select uuid, inventor_id from patent_20191231.rawinventor order by uuid limit 5 offset 0\n",
      "[('0000hccb98m2kc6g1v7128k5w', '20191231', '4341225-2'), ('0000kwt5abwdu9f4av6zoa61t', '20191231', '4339721-2'), ('0000n6xqianutadbzbgzwled7', '20191231', '6610738-6'), ('0000n8nqsxhrztn7djlxou00k', '20191231', '6448562-2'), ('0000p6jf5l8yzv04wimaoabab', '20191231', '4127345-1')]\n",
      "                        uuid database_update inventor_id\n",
      "0  0000hccb98m2kc6g1v7128k5w        20191231   4341225-2\n",
      "1  0000kwt5abwdu9f4av6zoa61t        20191231   4339721-2\n",
      "2  0000n6xqianutadbzbgzwled7        20191231   6610738-6\n",
      "3  0000n8nqsxhrztn7djlxou00k        20191231   6448562-2\n",
      "4  0000p6jf5l8yzv04wimaoabab        20191231   4127345-1\n"
     ]
    }
   ],
   "source": [
    "# Syntax testing\n",
    "limit = 5 \n",
    "offset = 0\n",
    "sql_stmt_template = \"select uuid, inventor_id from {0}.{1} order by uuid limit {2} offset {3}\".format(new_db, 'rawinventor', limit, offset)\n",
    "print(sql_stmt_template)\n",
    "result = db_con.execute(sql_stmt_template)\n",
    "# row needs to be uuid, database, inventor id\n",
    "chunk_results = [ (r[0], new_db_timestamp, r[1]) for r in result]\n",
    "print(chunk_results)\n",
    "\n",
    "chunk_df = pd.DataFrame(chunk_results, columns = pid_long_cols)\n",
    "print(chunk_df.head())\n",
    "\n",
    "# for pair in chunk_results:\n",
    "#     uuid = pair[0]\n",
    "#     inventor_id = pair[1]\n",
    "\n",
    "#     insert_row = (uuid, new_db_timestamp, inventor_id)\n",
    "#     insert_stmt_template = \"insert into {0}.{1} (uuid, database_update, inventor_id) values {2}\".format(new_db,'persistent_inventor_disambig_long',insert_row)\n",
    "#     print(insert_stmt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. rawinventor rows from current db update: insert into persistent_inventor_disambig_long table in chunks\n",
    "limit = 300000\n",
    "offset = 0\n",
    "\n",
    "start = time.time()\n",
    "itr = 0\n",
    "\n",
    "print('Estimated # of rounds: ', total_rows/300000)\n",
    "\n",
    "while True:\n",
    "    print('###########################################\\n')\n",
    "    print('Next iteration... ', itr)\n",
    "    \n",
    "    sql_stmt_template = \"select uuid, inventor_id from {0}.{1} order by uuid limit {2} offset {3};\".format(new_db, 'rawinventor', limit, offset)\n",
    "    print(sql_stmt_template)\n",
    "    result = db_con.execute(sql_stmt_template)\n",
    "    \n",
    "    # r = tuples of (uuid, inventor_id)\n",
    "    chunk_results = [ (r[0], new_db_timestamp, r[1]) for r in result]\n",
    "\n",
    "    # means we have no more result batches to process! done\n",
    "    if len(chunk_results) == 0:\n",
    "        break\n",
    "        \n",
    "    chunk_df = pd.DataFrame(chunk_results, columns = pid_long_cols)\n",
    "    chunk_df.to_csv(outfile_name_newinventors, index=False, header=False, mode = 'a', sep='\\t')\n",
    "    \n",
    "    # test \n",
    "    if offset == 600000:\n",
    "        break\n",
    "\n",
    "         \n",
    "    # continue\n",
    "    offset+=limit \n",
    "    itr+=1\n",
    "\n",
    "    if itr == 1:\n",
    "        print('Time for 1 iteration: ', time.time() - start, ' seconds')\n",
    "    print('###########################################\\n')\n",
    "\n",
    "    \n",
    "print('###########################################')\n",
    "print('total time taken:', round(time.time() - start, 2), ' seconds')\n",
    "print('###########################################')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T20:34:02.163400Z",
     "start_time": "2020-01-27T20:34:02.145277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['current_uuid', 'old_uuid', 'database_update', 'inventor_id']\n"
     ]
    }
   ],
   "source": [
    "# 3. get column names of persistent_inventor_disambig_long table for pivoting long -> wide\n",
    "result = db_con.execute(\"select column_name from information_schema.columns where table_schema = '{0}' and table_name = '{1}'\".format(new_db, 'persistent_inventor_disambig_long'))\n",
    "result_cols = [r[0] for r in result]\n",
    "pid_long_cols = [x for x in result_cols if x != 'id']\n",
    "pid_long_cols = ['current_uuid', 'old_uuid', 'database_update', 'inventor_id']\n",
    "print(pid_long_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T20:32:02.875336Z",
     "start_time": "2020-01-27T20:31:40.929740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select lf.uuid as current_uuid, ri_old.uuid as old_uuid, lf.database_update, lf.inventor_id from ( select uuid from patent_20191231.rawinventor order by uuid limit 5 offset 17000000) ri left join patent_20191231.persistent_inventor_disambig_long lf on ri.uuid = lf.uuid left join patent_20191008.rawinventor ri_old on lf.uuid = ri_old.uuid;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('z41yt85v6br6x22t4tgz0j4ej', 'z41yt85v6br6x22t4tgz0j4ej', '20181127', '10015597-1'),\n",
       " ('z41yt85v6br6x22t4tgz0j4ej', 'z41yt85v6br6x22t4tgz0j4ej', '20190312', '6624768-2'),\n",
       " ('z41yt85v6br6x22t4tgz0j4ej', 'z41yt85v6br6x22t4tgz0j4ej', '20190820', '6624768-2'),\n",
       " ('z41yt85v6br6x22t4tgz0j4ej', 'z41yt85v6br6x22t4tgz0j4ej', '20191008', '6624768-2'),\n",
       " ('z41yt85v6br6x22t4tgz0j4ej', 'z41yt85v6br6x22t4tgz0j4ej', '20191231', '6624768-2'),\n",
       " ('z41yvs6q9vhkv9s91b160yjh6', 'z41yvs6q9vhkv9s91b160yjh6', '20170808', '4980409-3'),\n",
       " ('z41yvs6q9vhkv9s91b160yjh6', 'z41yvs6q9vhkv9s91b160yjh6', '20171003', '4980409-3'),\n",
       " ('z41yvs6q9vhkv9s91b160yjh6', 'z41yvs6q9vhkv9s91b160yjh6', '20171226', '4980409-3'),\n",
       " ('z41yvs6q9vhkv9s91b160yjh6', 'z41yvs6q9vhkv9s91b160yjh6', '20180528', '4980409-3'),\n",
       " ('z41yvs6q9vhkv9s91b160yjh6', 'z41yvs6q9vhkv9s91b160yjh6', '20181127', '4980409-3'),\n",
       " ('z41yvs6q9vhkv9s91b160yjh6', 'z41yvs6q9vhkv9s91b160yjh6', '20190312', '4980409-3'),\n",
       " ('z41yvs6q9vhkv9s91b160yjh6', 'z41yvs6q9vhkv9s91b160yjh6', '20190820', '4980409-3'),\n",
       " ('z41yvs6q9vhkv9s91b160yjh6', 'z41yvs6q9vhkv9s91b160yjh6', '20191008', '4980409-3'),\n",
       " ('z41yvs6q9vhkv9s91b160yjh6', 'z41yvs6q9vhkv9s91b160yjh6', '20191231', '4980409-3'),\n",
       " ('z41z34y0k2jpwdgcssz2yxqfz', 'z41z34y0k2jpwdgcssz2yxqfz', '20170808', '4004299-1'),\n",
       " ('z41z34y0k2jpwdgcssz2yxqfz', 'z41z34y0k2jpwdgcssz2yxqfz', '20171003', '4004299-1'),\n",
       " ('z41z34y0k2jpwdgcssz2yxqfz', 'z41z34y0k2jpwdgcssz2yxqfz', '20171226', '4004299-1'),\n",
       " ('z41z34y0k2jpwdgcssz2yxqfz', 'z41z34y0k2jpwdgcssz2yxqfz', '20180528', '4004299-1'),\n",
       " ('z41z34y0k2jpwdgcssz2yxqfz', 'z41z34y0k2jpwdgcssz2yxqfz', '20181127', '4004299-1'),\n",
       " ('z41z34y0k2jpwdgcssz2yxqfz', 'z41z34y0k2jpwdgcssz2yxqfz', '20190312', '4004299-1'),\n",
       " ('z41z34y0k2jpwdgcssz2yxqfz', 'z41z34y0k2jpwdgcssz2yxqfz', '20190820', '4004299-1'),\n",
       " ('z41z34y0k2jpwdgcssz2yxqfz', 'z41z34y0k2jpwdgcssz2yxqfz', '20191008', '4004299-1'),\n",
       " ('z41z34y0k2jpwdgcssz2yxqfz', 'z41z34y0k2jpwdgcssz2yxqfz', '20191231', '4004299-1'),\n",
       " ('z41z3h2jzybrw7py3dmvez6tc', 'z41z3h2jzybrw7py3dmvez6tc', '20170808', '4702619-3'),\n",
       " ('z41z3h2jzybrw7py3dmvez6tc', 'z41z3h2jzybrw7py3dmvez6tc', '20171003', '4702619-3'),\n",
       " ('z41z3h2jzybrw7py3dmvez6tc', 'z41z3h2jzybrw7py3dmvez6tc', '20171226', '4702619-3'),\n",
       " ('z41z3h2jzybrw7py3dmvez6tc', 'z41z3h2jzybrw7py3dmvez6tc', '20180528', '4702619-3'),\n",
       " ('z41z3h2jzybrw7py3dmvez6tc', 'z41z3h2jzybrw7py3dmvez6tc', '20181127', '4702619-3'),\n",
       " ('z41z3h2jzybrw7py3dmvez6tc', 'z41z3h2jzybrw7py3dmvez6tc', '20190312', '4702619-3'),\n",
       " ('z41z3h2jzybrw7py3dmvez6tc', 'z41z3h2jzybrw7py3dmvez6tc', '20190820', '4702619-3'),\n",
       " ('z41z3h2jzybrw7py3dmvez6tc', 'z41z3h2jzybrw7py3dmvez6tc', '20191008', '4702619-3'),\n",
       " ('z41z3h2jzybrw7py3dmvez6tc', 'z41z3h2jzybrw7py3dmvez6tc', '20191231', '4702619-3'),\n",
       " ('z41z6xyn2nts8bicnad3k1uow', 'z41z6xyn2nts8bicnad3k1uow', '20170808', '5954602-2'),\n",
       " ('z41z6xyn2nts8bicnad3k1uow', 'z41z6xyn2nts8bicnad3k1uow', '20171003', '5954602-2'),\n",
       " ('z41z6xyn2nts8bicnad3k1uow', 'z41z6xyn2nts8bicnad3k1uow', '20171226', '5954602-2'),\n",
       " ('z41z6xyn2nts8bicnad3k1uow', 'z41z6xyn2nts8bicnad3k1uow', '20180528', '5954602-2'),\n",
       " ('z41z6xyn2nts8bicnad3k1uow', 'z41z6xyn2nts8bicnad3k1uow', '20181127', '5954602-2'),\n",
       " ('z41z6xyn2nts8bicnad3k1uow', 'z41z6xyn2nts8bicnad3k1uow', '20190312', '5954602-2'),\n",
       " ('z41z6xyn2nts8bicnad3k1uow', 'z41z6xyn2nts8bicnad3k1uow', '20190820', '5954602-2'),\n",
       " ('z41z6xyn2nts8bicnad3k1uow', 'z41z6xyn2nts8bicnad3k1uow', '20191008', '5954602-2'),\n",
       " ('z41z6xyn2nts8bicnad3k1uow', 'z41z6xyn2nts8bicnad3k1uow', '20191231', '5954602-2')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Syntax testing\n",
    "limit = 5\n",
    "offset = 17000000\n",
    "sql_stmt_inner = \"( select uuid from {0}.{1} order by uuid limit {2} offset {3}) ri\".format(new_db, 'rawinventor',  limit, offset)\n",
    "sql_stmt_template = \"select lf.uuid as current_uuid, ri_old.uuid as old_uuid, lf.database_update, lf.inventor_id from {0} left join {1}.{2} lf on ri.uuid = lf.uuid left join {3}.{4} ri_old on lf.uuid = ri_old.uuid;\".format(sql_stmt_inner,new_db,'persistent_inventor_disambig_long', old_db, 'rawinventor')\n",
    "\n",
    "print(sql_stmt_template)\n",
    "result = db_con.execute(sql_stmt_template)\n",
    "\n",
    "chunk_results = [r for r in result]\n",
    "chunk_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T20:43:18.066441Z",
     "start_time": "2020-01-27T20:43:18.035900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                current_uuid                   old_uuid  \\\n",
      "0  z41yt85v6br6x22t4tgz0j4ej  z41yt85v6br6x22t4tgz0j4ej   \n",
      "1  z41yt85v6br6x22t4tgz0j4ej  z41yt85v6br6x22t4tgz0j4ej   \n",
      "2  z41yt85v6br6x22t4tgz0j4ej  z41yt85v6br6x22t4tgz0j4ej   \n",
      "3  z41yt85v6br6x22t4tgz0j4ej  z41yt85v6br6x22t4tgz0j4ej   \n",
      "4  z41yt85v6br6x22t4tgz0j4ej  z41yt85v6br6x22t4tgz0j4ej   \n",
      "\n",
      "               database_update inventor_id  \n",
      "0  disamb_inventor_id_20181127  10015597-1  \n",
      "1  disamb_inventor_id_20190312   6624768-2  \n",
      "2  disamb_inventor_id_20190820   6624768-2  \n",
      "3  disamb_inventor_id_20191008   6624768-2  \n",
      "4  disamb_inventor_id_20191231   6624768-2  \n",
      "######################################################################\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of passed values is 41, index implies 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-61cba2164b9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# reset index to get back uuid as column, rename axis to get rid of database_update axis value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mchunk_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'current_uuid'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'old_uuid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'database_update'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inventor_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.reset_index().rename_axis(None,1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mchunk_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# post pivot & processing check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mpivot\u001b[0;34m(self, index, columns, values)\u001b[0m\n\u001b[1;32m   5192\u001b[0m         \"\"\"\n\u001b[1;32m   5193\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5194\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5196\u001b[0m     _shared_docs['pivot_table'] = \"\"\"\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/reshape/reshape.py\u001b[0m in \u001b[0;36mpivot\u001b[0;34m(self, index, columns, values)\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             indexed = self._constructor_sliced(self[values].values,\n\u001b[0;32m--> 414\u001b[0;31m                                                index=index)\n\u001b[0m\u001b[1;32m    415\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindexed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    260\u001b[0m                             \u001b[0;34m'Length of passed values is {val}, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                             \u001b[0;34m'index implies {ind}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                             .format(val=len(data), ind=len(index)))\n\u001b[0m\u001b[1;32m    263\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of passed values is 41, index implies 2"
     ]
    }
   ],
   "source": [
    "# Syntax testing\n",
    "chunk_df = pd.DataFrame(chunk_results, columns = pid_long_cols)\n",
    "chunk_df['database_update'] = 'disamb_inventor_id_' + chunk_df['database_update']\n",
    "\n",
    "# pre pivot check\n",
    "print(chunk_df.head())\n",
    "print('######################################################################\\n')\n",
    "\n",
    "# reset index to get back uuid as column, rename axis to get rid of database_update axis value\n",
    "chunk_df = chunk_df.pivot(index=['current_uuid','old_uuid'], columns='database_update', values='inventor_id')#.reset_index().rename_axis(None,1)\n",
    "chunk_df\n",
    "# post pivot & processing check\n",
    "#print(chunk_df.head())    \n",
    "#print('######################################################################\\n')\n",
    "\n",
    "# sort = False will preserve col order\n",
    "# formatted_chunk_df = pd.concat([header_df, chunk_df], sort=False)\n",
    "\n",
    "# # replace NAs with empty strings\n",
    "# formatted_chunk_df.to_csv(outfile_name, index=False, header=False, mode = 'a', sep='\\t', na_rep = None)\n",
    "\n",
    "# formatted_chunk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T17:03:14.036269Z",
     "start_time": "2020-01-27T17:03:14.030984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/pipeline_datadir/upload_20191231/disambig_output/persistent_inventor_wide.tsv'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-27T17:03:31.437414Z",
     "start_time": "2020-01-27T17:03:31.412839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        uuid disamb_inventor_id_20171226  \\\n",
      "0  0000hccb98m2kc6g1v7128k5w                         NaN   \n",
      "1  0000kwt5abwdu9f4av6zoa61t                         NaN   \n",
      "2  0000n6xqianutadbzbgzwled7                         NaN   \n",
      "3  0000n8nqsxhrztn7djlxou00k                         NaN   \n",
      "4  0000p6jf5l8yzv04wimaoabab                         NaN   \n",
      "\n",
      "  disamb_inventor_id_20171003 disamb_inventor_id_20170808  \\\n",
      "0                         NaN                         NaN   \n",
      "1                         NaN                         NaN   \n",
      "2                         NaN                         NaN   \n",
      "3                         NaN                         NaN   \n",
      "4                         NaN                         NaN   \n",
      "\n",
      "  disamb_inventor_id_20180528 disamb_inventor_id_20181127  \\\n",
      "0                         NaN                         NaN   \n",
      "1                         NaN                         NaN   \n",
      "2                         NaN                         NaN   \n",
      "3                         NaN                         NaN   \n",
      "4                         NaN                         NaN   \n",
      "\n",
      "  disamb_inventor_id_20190312 disamb_inventor_id_20190820  \\\n",
      "0                         NaN                         NaN   \n",
      "1                         NaN                         NaN   \n",
      "2                         NaN                         NaN   \n",
      "3                         NaN                         NaN   \n",
      "4                         NaN                         NaN   \n",
      "\n",
      "  disamb_inventor_id_20191008 disamb_inventor_id_20191231  \n",
      "0                         NaN                   4341225-2  \n",
      "1                         NaN                   4339721-2  \n",
      "2                         NaN                   6610738-6  \n",
      "3                         NaN                   6448562-2  \n",
      "4                         NaN                   4127345-1  \n"
     ]
    }
   ],
   "source": [
    "# Syntax testing\n",
    "# test = pd.concat([chunk_df_p1, chunk_df])\n",
    "# test.shape # should have 10 rows\n",
    "\n",
    "# if this works, then write to output file as append - should put in same way?\n",
    "# concat to header each time for organizing ---- then append to file \n",
    "\n",
    "#print(test)\n",
    "\n",
    "# test[test.isna()] = ''\n",
    "# test\n",
    "#print(header_df)\n",
    "test2 = pd.concat([header_df, chunk_df], sort=False)\n",
    "\n",
    "#test2\n",
    "\n",
    "#test3 = \n",
    "#header_df\n",
    "print(test2)\n",
    "\n",
    "test2.to_csv(outfile_name, index=False, header=False, mode = 'a', sep='\\t', na_rep = None)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Need to convert long table into wide and output .tsv - grabbing all uuid rows together for the set of uuids\n",
    "limit = 300000\n",
    "offset = 0\n",
    "\n",
    "start = time.time()\n",
    "itr = 0\n",
    "\n",
    "print('Estimated # of rounds: ', total_rows/300000)\n",
    "\n",
    "while True:\n",
    "    print('###########################################\\n')\n",
    "    print('Next iteration... ', itr)\n",
    "    \n",
    "    sql_stmt_inner = \"( select uuid from {0}.{1} order by uuid limit {2} offset {3}) ri\".format(new_db, 'rawinventor',  limit, offset)\n",
    "    sql_stmt_template = \"select lf.uuid, lf.database_update, lf.inventor_id from {0} inner join {1}.{2} lf on ri.uuid = lf.uuid;\".format(sql_stmt_inner,new_db,'persistent_inventor_disambig_long')\n",
    "\n",
    "    print(sql_stmt_template)\n",
    "    result = db_con.execute(sql_stmt_template)\n",
    "\n",
    "    chunk_results = [r for r in result]\n",
    "    print(len(chunk_results))\n",
    "    \n",
    "    # means we have no more result batches to process! done\n",
    "    if len(chunk_results) == 0:\n",
    "        break\n",
    "\n",
    "    chunk_df = pd.DataFrame(chunk_results, columns = pid_long_cols)\n",
    "    chunk_df['database_update'] = 'disamb_inventor_id_' + chunk_df['database_update']\n",
    "    \n",
    "    # pre pivot check\n",
    "    print(chunk_df.head())\n",
    "    print('######################################################################\\n')\n",
    "    \n",
    "    # reset index to get back uuid as column, rename axis to get rid of database_update axis value\n",
    "    chunk_df = chunk_df.pivot(index='uuid', columns='database_update', values='inventor_id').reset_index().rename_axis(None,1)\n",
    "    \n",
    "    # post pivot & processing check\n",
    "    print(chunk_df.head())    \n",
    "    print('######################################################################\\n')\n",
    "\n",
    "    # sort = False will preserve col order\n",
    "    formatted_chunk_df = pd.concat([header_df, chunk_df], sort=False)\n",
    "    \n",
    "    # replace NAs with empty strings\n",
    "    formatted_chunk_df.to_csv(outfile_name, index=False, header=False, mode = 'a', sep='\\t', na_rep = None)\n",
    "    \n",
    "    offset+=limit \n",
    "    itr+=1\n",
    "\n",
    "    if itr == 1:\n",
    "        print('Time for 1 iteration: ', time.time() - start, ' seconds')\n",
    "    print('###########################################\\n')\n",
    "    \n",
    "    \n",
    "print('###########################################')\n",
    "print('total time taken:', round(time.time() - start, 2), ' seconds')\n",
    "print('###########################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. load wide table \n",
    "#LOAD DATA INFILE 'persistent_inventor_long.tsv' INTO TABLE persistent_inventor_wide FIELDS TERMINATED BY '\\t' (uuid, database_update, inventor_id) IGNORE 1 lines;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

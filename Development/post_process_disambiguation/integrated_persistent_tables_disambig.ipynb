{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from warnings import filterwarnings\n",
    "import csv\n",
    "import re, os, random, string, codecs\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "import time\n",
    "import tqdm\n",
    "project_home = os.environ['PACKAGE_HOME']\n",
    "from Development.helpers import general_helpers\n",
    "import argparse\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# 1. get column names of persistent_inventor_disambig_long table & create output file\n",
    "\n",
    "# REQUIRES: db_con, new_db, long table name\n",
    "# MODIFIES: nothing\n",
    "# EFFECTS: returns list of long persistent entity columns\n",
    "def get_long_entity_cols(db_con, new_db, persistent_long_table):\n",
    "    \n",
    "    result = db_con.execute(\"select column_name from information_schema.columns where table_schema = '{0}' and table_name = '{1}';\".format(new_db, persistent_long_table))\n",
    "    result_cols = [r[0] for r in result]\n",
    "    # skip auto-increment column, we don't need it\n",
    "    long_cols = [x for x in result_cols if x != 'id']\n",
    "    print(long_cols)\n",
    "    \n",
    "    return long_cols\n",
    "\n",
    "# REQUIRES: db_con, new_db, raw entity table name, id_col (assignee/inventor id), total rows in raw entity table, new_db timestamp, outfile path, long table header cols\n",
    "# MODIFIES: nothing\n",
    "# EFFECTS: returns list of long persistent entity columns\n",
    "def write_long_outfile_newrows(db_con, new_db, raw_table, id_col, total_rows, new_db_timestamp, outfile_fp, header_long):\n",
    "    \n",
    "    ############# 2.output rawinventor rows from newdb\n",
    "    limit = 300000\n",
    "    offset = 0\n",
    "\n",
    "    start = time.time()\n",
    "    itr = 0\n",
    "\n",
    "    print('Estimated # of rounds: ', total_rows/300000)\n",
    "\n",
    "    while True:\n",
    "        print('###########################################\\n')\n",
    "        print('Next iteration... ', itr)\n",
    "\n",
    "        sql_stmt_template = \"select uuid, {0} from {1}.{2} order by uuid limit {3} offset {4};\".format(id_col,new_db, raw_table, limit, offset)\n",
    "\n",
    "        print(sql_stmt_template)\n",
    "        result = db_con.execute(sql_stmt_template)\n",
    "\n",
    "        # r = tuples of (uuid, new db timestamp, entity id)\n",
    "        chunk_results = [(r[0], new_db_timestamp, r[1]) for r in result]\n",
    "\n",
    "        # means we have no more result batches to process! done\n",
    "        if len(chunk_results) == 0:\n",
    "            break\n",
    "\n",
    "        chunk_df = pd.DataFrame(chunk_results, columns = header_long)\n",
    "        chunk_df.to_csv(outfile_fp, index=False, header=False, mode = 'a', sep='\\t')\n",
    "\n",
    "        # continue\n",
    "        offset+=limit \n",
    "        itr+=1\n",
    "\n",
    "        if itr == 1:\n",
    "            print('Time for 1 iteration: ', time.time() - start, ' seconds')\n",
    "            print('###########################################\\n')\n",
    "\n",
    "\n",
    "    print('###########################################')\n",
    "    print('total time taken:', round(time.time() - start, 2), ' seconds')\n",
    "    print('###########################################')\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRES: db_con, old_db, new_db, entity, raw entity table name, total rows in raw entity table, new_db timestamp\n",
    "# MODIFIES: nothing\n",
    "# EFFECTS: returns list of wide persistent entity columns\n",
    "def get_wide_entity_disambig_cols(db_con, old_db, persistent_disambig_table):\n",
    "    \n",
    "    result = db_con.execute(\"select column_name from information_schema.columns where table_schema = '{0}' and table_name = '{1}';\".format(old_db, persistent_disambig_table))\n",
    "    result_cols = [r[0] for r in result]\n",
    "    disambig_cols = [x for x in result_cols if x.startswith('disamb')]\n",
    "    disambig_cols.sort()\n",
    "    print(disambig_cols)\n",
    "    \n",
    "    return disambig_cols\n",
    "\n",
    "# REQUIRES: db_con, old_db, new_db, entity, raw entity table name, total rows in raw entity table, new_db timestamp\n",
    "# MODIFIES: nothing\n",
    "# EFFECTS: writes .tsv of table in wide format\n",
    "def write_wide_outfile(db_con, new_db, entity, persistent_long_table, raw_table, id_col, total_rows, new_db_timestamp, outfile_fp, header_df):\n",
    "    \n",
    "    # fixed\n",
    "    current_rawentity = 'current_raw{0}_id'.format(entity)\n",
    "    old_rawentity = 'old_raw{0}_id'.format(entity)\n",
    "    disamb_str = 'disamb_{}_id_'.format(entity)\n",
    "    \n",
    "    # fixed\n",
    "    chunk_cols = [current_rawentity, old_rawentity, 'database_update', id_col]\n",
    "\n",
    "\n",
    "    ############ 2. Convert long -> wide and output .tsv: grab all uuid rows together for a set of uuids\n",
    "    limit = 300000\n",
    "    offset = 0\n",
    "\n",
    "    start = time.time()\n",
    "    itr = 0\n",
    "    \n",
    "    print('Estimated # of rounds: ', total_rows/300000)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        print('###########################################\\n')\n",
    "\n",
    "        print('Next iteration... ', itr)\n",
    "\n",
    "        sql_stmt_inner = \"(select uuid from {0}.{1} order by uuid limit {2} offset {3}) raw\".format(new_db, raw_table, limit, offset)\n",
    "        sql_stmt_template = \"select lf.uuid as {0}, raw_old.uuid as {1}, lf.database_update, lf.{2} from {3} left join {4}.{5} lf on raw.uuid = lf.uuid left join {5}.{6} raw_old on lf.uuid = raw_old.uuid;\".format(current_rawentity, old_rawentity, id_col, sql_stmt_inner, new_db, persistent_long_table, old_db, raw_table)\n",
    "\n",
    "        print(sql_stmt_template)\n",
    "        result = db_con.execute(sql_stmt_template)\n",
    "\n",
    "        chunk_results = [r for r in result]\n",
    "\n",
    "        # no more result batches to process! done\n",
    "        if len(chunk_results) == 0:\n",
    "            break\n",
    "\n",
    "        # 0. Preprocess dataupdate column to add prefix + save current/old uuid lookup\n",
    "        chunk_df = pd.DataFrame(chunk_results, columns = chunk_cols)\n",
    "        chunk_df['database_update'] = disamb_str + chunk_df['database_update']\n",
    "\n",
    "        uuid_lookup = chunk_df[[current_rawentity, old_rawentity]].drop_duplicates()\n",
    "\n",
    "        # 1. Pivot, reset index & get back uuid as column, rename axis & remove database_update axis value\n",
    "        pivoted_chunk_df = chunk_df.pivot(index=current_rawentity, columns='database_update', values=id_col).reset_index().rename_axis(None,1)\n",
    "\n",
    "        # 2. Merge back old rawinventor id column\n",
    "        merged_df = pd.merge(pivoted_chunk_df,uuid_lookup)\n",
    "\n",
    "        # 3. Concat with sort = False (preserves desired col order from header_df)\n",
    "        formatted_chunk_df = pd.concat([header_df, merged_df], sort=False)\n",
    "\n",
    "        # 4. Write to outfile\n",
    "        formatted_chunk_df.to_csv(outfile_fp, index=False, header=False, mode = 'a', sep='\\t', na_rep = None)\n",
    "\n",
    "        offset+=limit \n",
    "        itr+=1\n",
    "\n",
    "        if itr == 1:\n",
    "        print('Time for 1 iteration: ', time.time() - start, ' seconds')\n",
    "        print('###########################################\\n')\n",
    "\n",
    "\n",
    "    print('###########################################\\n')\n",
    "    print('total time taken:', round(time.time() - start, 2), ' seconds')\n",
    "    print('###########################################\\n')\n",
    "\n",
    "    return\n",
    "\n",
    "# REQUIRES: entity, varchar, entity database id cols, sql create statement\n",
    "# MODIFIES: nothing\n",
    "# EFFECTS: creates persistent entity table syntax\n",
    "def get_create_syntax(db_con, entity, entity_db_cols, create_stmt):\n",
    "    \n",
    "    current_rawentity = 'current_raw{0}_id'.format(entity)\n",
    "    old_rawentity = 'old_raw{0}_id'.format(entity)\n",
    "    \n",
    "    \n",
    "    # loop through to construct create syntax for entity disambig cols\n",
    "    for col in entity_db_cols:\n",
    "\n",
    "        # regardless of entity, uuid column fixed at 32\n",
    "        if col == current_rawentity or col == old_rawentity:\n",
    "            add_col_stmt = \"`{0}` varchar(32),\".format(col)\n",
    "        \n",
    "        # if assignee is entity - then disambig cols need to be varchar 64\n",
    "        else if entity == 'assignee':\n",
    "            add_col_stmt = \"`{0}` varchar(64), \".format(col)\n",
    "        \n",
    "        # inventor entity -  disambig cols need to be varchar 16\n",
    "        else:\n",
    "            add_col_stmt = \"`{0}` varchar(16), \".format(col)\n",
    "\n",
    "        create_stmt += add_col_stmt\n",
    "\n",
    "    return create_stmt\n",
    "\n",
    "\n",
    "# REQUIRES: db_con, entity, persistent entity table, outfile folder path\n",
    "# MODIFIES: nothing\n",
    "# EFFECTS: creates persistent entity table for new database\n",
    "def create_wide_table_database(db_con, entity, persistent_disambig_table, outfile_fp):\n",
    "        \n",
    "    ####### 3. create table in database\n",
    "    db_con.execute('drop table if exists {}.{}'.format(new_db, persistent_disambig_table)\n",
    "\n",
    "    current_rawentity = 'current_raw{0}_id'.format(entity)\n",
    "                   \n",
    "    # only read header for creating table\n",
    "    wide_df = pd.read_csv(outfile_fp, sep='\\t', nrows = 1)\n",
    "    entity_db_cols = list(wide_df.columns.values)\n",
    "\n",
    "    create_stmt = 'create table {0}.{1} ( '.format(new_db, persistent_disambig_table)\n",
    "    primary_key_stmt = 'PRIMARY KEY (`{0}`));'.format(current_rawentity)\n",
    "                   \n",
    "                   \n",
    "    create_stmt = get_create_syntax(db_con, entity, entity_db_cols, create_stmt)\n",
    "\n",
    "    create_stmt = create_stmt + primary_key_stmt\n",
    "    print(create_stmt)\n",
    "    db_con.execute(create_stmt)\n",
    "                   \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser(description= \"persistent table creation\")\n",
    "    required_named = parser.add_argument_group('required args')\n",
    "    required_named.add_argument('-e', type=str, nargs=1, help='The persistent entity', required=True)\n",
    "    \n",
    "    try:\n",
    "        args = parser.parse_args()\n",
    "    except:\n",
    "        parser.print_help()\n",
    "        exit(1)\n",
    "\n",
    "    entity = args.e[0]\n",
    "    \n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(project_home + '/Development/config.ini')\n",
    "\n",
    "    db_con = general_helpers.connect_to_db(config['DATABASE']['HOST'], config['DATABASE']['USERNAME'],\n",
    "                                           config['DATABASE']['PASSWORD'], config['DATABASE']['NEW_DB'])\n",
    "\n",
    "    disambig_folder = \"{}/disambig_output/\".format(config['FOLDERS']['WORKING_FOLDER'])\n",
    "    outfile_name_long = 'persistent_{0}_long_{1}.tsv'.format(entity, new_db_timestamp)\n",
    "    outfile_fp_long = disambig_folder + outfile_name_long\n",
    "\n",
    "    old_db = config['DATABASE']['OLD_DB']\n",
    "    new_db = config['DATABASE']['NEW_DB']\n",
    "    new_db_timestamp = new_db.replace('patent_', '')\n",
    "    \n",
    "    # set of values that change depending on entity\n",
    "    persistent_long_table = 'persistent_{0}_disambig_long'.format(entity)\n",
    "    raw_table = 'raw{0}'.format(entity)\n",
    "    id_col = '{0}_id'.entity\n",
    "    \n",
    "    header_long = get_long_entity_cols(db_con,new_db,persistent_long_table)\n",
    "    \n",
    "    # generate header for output file\n",
    "    header_df = pd.DataFrame(columns = header_long)\n",
    "    header_df.to_csv(outfile_fp_long, index=False, header=True, sep='\\t')\n",
    "\n",
    "    # get total rows in raw entity table\n",
    "    result = db_con.execute('select count(*) from {0}.{1}'.format(new_db, raw_table))\n",
    "    total_rows = [r[0] for r in result][0]\n",
    "    \n",
    "    write_long_outfile_newrows(db_con, new_db, raw_table, id_col, total_rows, new_db_timestamp, outfile_fp_long, header_long)\n",
    "    \n",
    "    \n",
    "    ######### 3. load data\n",
    "    db_con.execute(\"LOAD DATA LOCAL INFILE '{0}' INTO TABLE {1}.{2} FIELDS TERMINATED BY '\\t' NULL DEFINED BY '' IGNORE 1 lines (uuid, database_update, {3});\".format(outfile_fp, new_db, persistent_long_table, id_col))\n",
    "\n",
    "    ################################################################################################\n",
    "    # PART 2\n",
    "    ################################################################################################\n",
    "\n",
    "    ######## 4. long -> wide\n",
    "    outfile_name_wide = 'persistent_{}_wide.tsv'.format(entity)\n",
    "    outfile_fp_wide = disambig_folder + outfile_name_wide\n",
    "    \n",
    "    persistent_disambig_table = 'persistent_{0}_disambig'.format(entity)\n",
    "    \n",
    "    # get disambig cols from old db's persistent_inventor_disambig\n",
    "    disambig_cols = get_wide_entity_disambig_cols(db_con, old_db, persistent_disambig_table)\n",
    "\n",
    "    # Add new column for this data update:\n",
    "    raw_cols = ['current_{0}_id'.format(raw_table), 'old_{0}_id'.format(raw_table)]\n",
    "    header_wide = [raw_cols[0], raw_cols[1]] + disambig_cols + ['disamb_{0}_id_'.format(entity) + new_db_timestamp]\n",
    "    print(header_wide)\n",
    "    header_df = pd.DataFrame(columns = header_wide)\n",
    "    header_df.to_csv(outfile_fp_wide, index=False, header=True, sep='\\t')\n",
    "    \n",
    "    write_wide_outfile(db_con, new_db, entity, persistent_long_table, raw_table, id_col, total_rows, new_db_timestamp, outfile_fp_wide, header_df)\n",
    "    \n",
    "    \n",
    "    ####### 3. create table in database\n",
    "    create_wide_table_database(db_con, entity, persistent_disambig_table, outfile_fp_wide)\n",
    "                             \n",
    "    ######### 4. load data\n",
    "    db_con.execute(\"LOAD DATA LOCAL INFILE '{0}' INTO TABLE {1}.{2} FIELDS TERMINATED BY '\\t' NULL DEFINED BY '' IGNORE 1 lines;\".format(outfile_fp_wide, new_db, persistent_disambig_table))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

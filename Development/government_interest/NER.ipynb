{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T19:48:32.200337Z",
     "start_time": "2019-06-09T19:48:31.473918Z"
    }
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "#### This file is a re-write of govtInterest_v1.0.pl\n",
    "#### input files: merged csvs, NER, omitLocs\n",
    "#### output files: \"NER_output.txt\",\"distinctOrgs.txt\"\n",
    "##################################################################\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "import subprocess\n",
    "import os\n",
    "from os import listdir\n",
    "import re \n",
    "from itertools import chain \n",
    "import string "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T19:48:33.078172Z",
     "start_time": "2019-06-09T19:48:33.002147Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_heading(gi_statement):\n",
    "    #get the all caps headings from the start of the GI statement\n",
    "    heading = list(filter(None, [x.strip() for x in re.findall(r\"\\b[A-Z\\s-]+\\b\", gi_statement)]))\n",
    "    if len(heading) > 0:\n",
    "        possible_heading = heading[0]\n",
    "        #only keep it if it is the start of the text and it isn't 'A': \n",
    "        if heading[0] == gi_statement[:len(heading[0])] and len(heading[0])>1:\n",
    "            return heading[0]\n",
    "    return ''\n",
    "\n",
    "def prepare_input_files(working_folder, merged_csv_output):\n",
    "    #input_data = '{}/parsed_data'.format(working_folder)\n",
    "    input_data = '{}/gi_nulls_reparsed'.format(working_folder)\n",
    "    all_gi_data = pd.DataFrame()\n",
    "    for folder in os.listdir(input_data):\n",
    "        all_gi_data = all_gi_data.append(pd.read_csv('{}/{}/government_interest.csv'.format(input_data, folder), delimiter = '\\t'))\n",
    "     \n",
    "    all_gi_data.reset_index(inplace=True)\n",
    "    all_gi_data['heading'] = all_gi_data['gi_statement'].apply(get_heading)\n",
    "    all_gi_data['gi_statement'] = all_gi_data.apply(lambda row : row['gi_statement'][len(row['heading']):], axis = 1)\n",
    "    all_gi_data = all_gi_data[['patent_id', 'heading', 'gi_statement']]\n",
    "    \n",
    "    \n",
    "    all_gi_data.to_csv(merged_csv_output, index = False)\n",
    "    return all_gi_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run & process NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T19:48:37.469136Z",
     "start_time": "2019-06-09T19:48:37.425059Z"
    }
   },
   "outputs": [],
   "source": [
    "# Requires: NER DB filepath, dataframe\n",
    "# Modifies: nothing\n",
    "# Effects: Run NER on gi_statements from dataframe\n",
    "def run_NER(fp, txt_fp_in, txt_fp_out, data, classif, classif_dirs ):\n",
    "\tos.chdir(fp)\n",
    "\t\n",
    "\tgi_stmt_full = data['gi_statement'].tolist()\n",
    "\t\n",
    "\t# Limit to how many lines java call can read for NER\n",
    "\tnerfc = 5000\n",
    "\n",
    "\t# Estimate # of files for all gi_statements to process\n",
    "\tnum_files = int(math.ceil(len(gi_stmt_full) / nerfc))\n",
    "\tprint(\"Number of input files needed for NER: \" + str(num_files))\n",
    "\t\n",
    "\t# Store name of input files for NER call\n",
    "\tinput_files = []\n",
    "\n",
    "\t# Chunk GI statements by nerfc limit\n",
    "\tgi_chunked = general_helpers.chunks(gi_stmt_full, nerfc)\n",
    "\t\n",
    "\tfor item in range(0, len(gi_chunked)):\n",
    "\t\twith open(txt_fp_in + str(item) + '_file.txt', 'w', encoding='utf-8') as f:\n",
    "\t\t \t\t\tgi_stmt_str = '\\n'.join(gi_chunked[item])\n",
    "\t\t \t\t\tf.write(gi_stmt_str)\n",
    "\n",
    "\t\t# Save file name\n",
    "\t\tinfile_name = str(item) + \"_file.txt\"\n",
    "\t\tinput_files.append(infile_name)\n",
    "\n",
    "\t\n",
    "\t# Run java call for NER\n",
    "\tfor cf in range(0,len(classif)):\n",
    "\t\tfor f in input_files:\t    \n",
    "\t\t\tcmd_pt1 = 'java -mx500m -classpath stanford-ner.jar edu.stanford.nlp.ie.crf.CRFClassifier'\n",
    "\t\t\tcmd_pt2 = '-loadClassifier ' + './' + classif[cf]\n",
    "\t\t\tcmd_pt3 = '-textFile ./in/' + f + ' -outputFormat inlineXML 2>> error.log'\n",
    "\t\t\tcmd_full = cmd_pt1 + ' ' + cmd_pt2 + ' ' + cmd_pt3\n",
    "\t\t\tcmdline_params = cmd_full.split()\n",
    "\t\t\tprint(cmdline_params)\n",
    "\t\t\t\n",
    "\t\t\twith open(txt_fp_out + classif_dirs[cf] + f, \"w\") as xml_out: \n",
    "\n",
    "\t\t\t\tsubprocess.run(cmdline_params, stdout=xml_out)\n",
    "\t\t\t\n",
    "\treturn\n",
    "\n",
    "\n",
    "# Requires: filepath, merged_df frame\n",
    "# Modifies: nothing\n",
    "# Effects: Process NER on merged_csvs, returns orgs list, locs list\n",
    "def process_NER(txt_fp_out, data):\n",
    "\tos.chdir(txt_fp_out)\n",
    "\tner_output = listdir(os.getcwd())\n",
    "\tprint(ner_output)\n",
    "\torgs_full_list = []\n",
    "\n",
    "\tfor f in ner_output:\n",
    "\t\twith open(f, \"r\") as output:\n",
    "\t\t\tcontent = output.readlines()\n",
    "\t\t\torgs_full_list = parse_xml_ner(orgs_full_list, content)\t\n",
    "\n",
    "\t# Flatten list of lists \n",
    "\tflat_orgs = [y for x in orgs_full_list for y in x]\n",
    "\torgs_final = set(flat_orgs)\n",
    "\n",
    "\n",
    "\treturn orgs_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add_cols function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T19:48:45.282147Z",
     "start_time": "2019-06-09T19:48:45.262869Z"
    }
   },
   "outputs": [],
   "source": [
    "# Requires: filepath, merged_df frame\n",
    "# Modifies: nothing\n",
    "# Effects: Process NER on merged_csvs, returns orgs list, locs list\n",
    "def add_cols(data, orgs):\n",
    "\n",
    "\tprint(\"Cleaning and Adding Columns...\")\n",
    "\t# Clean organizations\n",
    "\torgs_final = clean_orgs(orgs)\n",
    "\t\n",
    "\tgi_statements = data['gi_statement'].tolist()\n",
    "\t\n",
    "\t# Add orgs column\n",
    "\tgi_all_orgs = []\n",
    "\tfor gi in gi_statements:\n",
    "\t\tgi_orgs = []\n",
    "\t\tfor org in orgs_final:\n",
    "\t\t\tif org in gi:\n",
    "\t\t\t\tgi_orgs.append(org)\n",
    "\n",
    "\t\t# Once full org list formed for gi, join\n",
    "\t\tgi_final = '|'.join(gi_orgs)\t\t\n",
    "\t\tgi_all_orgs.append(gi_final)\n",
    "\n",
    "\t\n",
    "\tgi_all_orgs = [x.lstrip('|') for x in gi_all_orgs]\n",
    "\tdata['orgs'] = pd.Series(gi_all_orgs)\n",
    "\n",
    "\n",
    "\t# Extract and clean Contract Numbers\n",
    "\tcontracts = clean_contracts(data, gi_statements)\n",
    "\t\n",
    "\t# Add contracts column for contracts\n",
    "\tdata['contracts'] = pd.Series(contracts)\n",
    "\n",
    "\treturn data, orgs_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T20:00:58.174153Z",
     "start_time": "2019-06-09T20:00:58.163328Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_output(output_fp, data, orgs):\n",
    "    print(\"Writing Output...\")\n",
    "    # Write out extracted NER output\n",
    "    # remove index column\n",
    "    data_final = data[['patent_id','heading','gi_statement','orgs','contracts']]\n",
    "    data_final.to_csv(output_fp + \"/NER_output.csv\",index=False)\n",
    "    \n",
    "    # write out distinct organizations\n",
    "    orgs.sort()\n",
    "    \n",
    "    with open(output_fp + \"/distinct_orgs.txt\", \"w\") as p:\n",
    "        for item in orgs:\n",
    "            p.write(str(item) + \"\\n\")\n",
    "            \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean orgs and contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T19:48:52.446243Z",
     "start_time": "2019-06-09T19:48:52.337759Z"
    }
   },
   "outputs": [],
   "source": [
    "#--------Helper Functions-------#\n",
    "# Requires: organizations list\n",
    "# Modifies: organizations list\n",
    "# Effects: clean organizations\n",
    "def clean_orgs(orgs):\n",
    "\t\n",
    "\t# Strip whitespace\n",
    "\torgs = [x.lstrip() for x in orgs]\n",
    "\torgs = [x.rstrip() for x in orgs]\n",
    "\n",
    "\t# Grant-related cleaning\n",
    "\torgs = [re.sub(\"Federally[\\-,\\s]Sponsored\\sResearch(\\sThis)?|Federal\\s(Contract|Government|Government Contract)|(Federal\\sGrant|Grant\\sNumber).+|Grant\\sNo\\.?.+|Grant\\s#.+|(and)?\\s?Grant\", \"\", x) for x in orgs]\n",
    "\n",
    "\t# Contract/Case related cleaning\n",
    "\torgs = [re.sub(\"Government\\sContract.+|Case?\\s(Number)?|Subcontract.+|and\\s(Contract)|(and)?\\s?Contract\\sNos?\\.?.+|Contract\\s[A-Z,\\d,\\-,a-z].+[\\d].+|Case\\sNo\\.?.+|(Contract\\sNumber|Contract\\s#).+|Order\\sNo|[C,c]ontract\\/|Case\"\n",
    "\t\t, \"\", x) for x in orgs]\n",
    "\t\n",
    "\t# Award/Agreement related cleaning\n",
    "\torgs = [re.sub(\"(Award|Agreement)\\sNumbers?.+|(Award|Agreement)\\sNos?\\.?.+|Award\\s[A-Z].+[\\d].+\\sand|Award\\s[A-Z,\\d,\\-,/]{10,30}|Award\", \"\", x) for x in orgs]\n",
    "\t\n",
    "\t# Misc. cleaning\n",
    "\torgs = [re.sub(\"Cooperative\\sAgreement.+\", \"Cooperative Agreement\", x) for x in orgs]\n",
    "\torgs = [re.sub(\"Agreement\\sNCRID-08-317-00\", \"Agreement\", x) for x in orgs]\n",
    "\torgs = [re.sub(\"Energy\\sContract.+\", \"Energy\", x) for x in orgs]\n",
    "\torgs = [re.sub(\"Development\\sInitiative\\s.+\", \"Development Initiative\", x) for x in orgs]\n",
    "\torgs = [re.sub(\"USEPA.+\", \"USEPA\", x) for x in orgs]\n",
    "\torgs = [re.sub(\"YFA.+\", \"YFA\", x) for x in orgs]\n",
    "\torgs = [re.sub(\"Work\\sUnit.+\", \"Work Unit\", x) for x in orgs]\n",
    "\torgs = [re.sub(\"Training\\sNumber.+|\\?|\\)|\\(|,\\.\", \"\", x) for x in orgs]\n",
    "\torgs = [re.sub(\"[A-Z,\\d,\\-]{10,30}|Agreement\\|(Number)?.+|And Contract|And$\",\"\", x) for x in orgs]\n",
    "\torgs = [re.sub(\"\\sNos?\\.?$|\\]|\\[|no\\.|&|;$|\\s1$|#|'|[#,A-Z][a-z,\\d,A-Z][a-z\\d][\\d]{3,10}\", \"\", x) for x in orgs]\n",
    "\torgs = [re.sub(\"Applications?\\sI[dD]?.+|Project\\s#|Project\\sNumber.+|Prime\\sContract|Merit\\sReview?.+|Merit\\sAward\", \"\", x) for x in orgs]\n",
    "\torgs = [re.sub(\"Goverment|Government\\sSupport\\s?(under)?|(NIH|NHI)\\s1|Health 1R43|U01\", \"\", x) for x in orgs]\n",
    "\torgs = [re.sub(\"Foundation\\sNumber|Foundation\\s[\\d]{5,15}|U01|R01|P\\.O\\.|Numbers?\", \"\", x) for x in orgs]\n",
    "\torgs = [re.sub(\"NIH[#/]\", \"NIH\", x) for x in orgs]\n",
    "\torgs = [x.lstrip() for x in orgs]\n",
    "\torgs = [x.rstrip() for x in orgs]\n",
    "\n",
    "\t# Remove Dups\n",
    "\torgs = set(orgs)\n",
    "\n",
    "\t# Additional general fields to remove\n",
    "\tto_remove = [\"national\",\"National\",\"National Science\", \"Agency\",\"U.S.C.\", \"Contract\", \"Invention\", \"Cooperative\",\"The Department\", \"The United States Government\" , \"National Institute\", \"National Institutes\",\"Federal\", \"RR\",\"National Institute of\",\"research\", \"Research\",\"US government\", \"U.S. Government\", \"US Government\", \"United\",\"United States Government\", \"United States Department\",\"United Stated\", \"United States\", \"U.S. Department\",\"U.S.C\", \"U.S.C\", \"Defense\", \"Merit\" ,\"Government\", \"U.S.\", \"USA\", \"s\", \"Department\"]\n",
    "\torgs = [x for x in orgs if x not in to_remove]\n",
    "\t\n",
    "\n",
    "\tfinal_output_dir = \"G:/PatentsView/cssip/PatentsView-DB/Development/government_interest/test_output/\"\n",
    "\t\n",
    "\treturn orgs\n",
    "\n",
    "# Requires: data dict\n",
    "# Modifies: nothing\n",
    "# Effects: clean giStatement field for certain contract #s, return data with\n",
    "#          contracts column\n",
    "# Note: look at this again, right now Bethesda & SD related only\n",
    "def clean_contracts(data, gi_statements):\n",
    "\n",
    "\tcontracts = []\n",
    "\t# STEP 1. Public Law - Don't need contract awards \n",
    "\tcontract_nums = data['gi_statement'].str.contains(\"Public Law\")\n",
    "\t\n",
    "\t# get index of law ones\n",
    "\tlaw_stmts = contract_nums[contract_nums].index\n",
    "\tlaw_stmts = law_stmts.tolist()\n",
    "\tsave_file = []\n",
    "\n",
    "\t\n",
    "\tfor gi in gi_statements:\n",
    "\t# STEP 2. Extract contract awards\n",
    "\t############################# Expression 1\n",
    "\t# [A-Za-z\\d] start with alphanumeric char\n",
    "\t# [A-Za-z\\d-] 2nd char alphanumeric or - \n",
    "\t# [^\\s] no spacing\n",
    "\t# [\\d] at least one more digit \n",
    "\t# [A-Za-z\\d-]+  finish with alphanumeric char or - 1 or more times\n",
    "\t############################ Expression 2 \n",
    "\t# [A-Z\\d]{1,3} - alphanumeric 1-3 times, capital A-Z only\n",
    "\t# \\s single space\n",
    "\t# [A-Z\\d-]+\\d - alphanumeric or - 1 or more times, followed by digit (redundant but stops\n",
    "\t# expression for case like \"IN AGREEMENT\")\n",
    "\n",
    "\t\tcontract_nums = re.findall(\"[A-Za-z\\d][A-Za-z\\d-]+[^\\s][\\d][A-Za-z\\d-]+|[A-Z\\d]{1,3}\\s[A-Z\\d-]+\\d\", gi)\n",
    "\n",
    "\t\tcontract_nums = '|'.join(contract_nums)\n",
    "\t\tcontracts.append(contract_nums)\n",
    "\n",
    "\tfor law_idx in law_stmts:\n",
    "\t\tcontracts[law_idx] = re.sub(\"((96|85)-\\d{3}|USC\\s\\d{3}|20\\d{3}|111-\\d{3})\\|?\", \"\", contracts[law_idx])\n",
    "\t\tsave_file.append(contracts[law_idx])\n",
    "\t\n",
    "\t# Clean up calif./bethesda codes\n",
    "\tca_be = data['gi_statement'].str.contains(\"Calif\\.|Bethesda\", regex=True)\n",
    "\t\n",
    "\t# Get index of calif./bethesda\n",
    "\tidx_cabe = ca_be[ca_be].index\n",
    "\t\n",
    "\tfor idx in idx_cabe:\n",
    "\n",
    "\t\tcontracts[idx] = re.sub(\"619\\)553-5118\\|?|619\\)553-5120\\|?|553-5118\\|?|(619-)?553-2778\\|?|92152\\|?|72120\\|?|20012\\|?|53510\\|?|D0012\\|?|53560\\|?|20014\\|?|20892\\|?\", \"\", contracts[idx])\n",
    "\n",
    "\tcontracts = [x.lstrip() for x in contracts]\n",
    "\tcontracts = [x.rstrip('|') for x in contracts]\n",
    "\t\n",
    "\treturn contracts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse xml from ner tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T19:48:54.679048Z",
     "start_time": "2019-06-09T19:48:54.659705Z"
    }
   },
   "outputs": [],
   "source": [
    "# Requires: organizations list, locations list, content\n",
    "# Modifies: nothing\n",
    "# Effects: parses XML file for orgs, locs\n",
    "def parse_xml_ner(orgs_full, content):\n",
    "\t\n",
    "\tfor line in content: \n",
    "\t\t\t\torgs = re.findall(\"<ORGANIZATION>[^<]+</ORGANIZATION>\", line)\n",
    "\t\t\t\torgs_clean = [re.sub(\"<ORGANIZATION>|</ORGANIZATION>\", \"\", x) for x in orgs]\n",
    "\t\t\t\torgs_full.append(orgs_clean)\n",
    "\n",
    "\n",
    "\treturn orgs_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-09T20:01:40.276136Z",
     "start_time": "2019-06-09T20:01:03.048016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input files needed for NER: 1\n",
      "['java', '-mx500m', '-classpath', 'stanford-ner.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', './classifiers/english.all.3class.distsim.crf.ser.gz', '-textFile', './in/0_file.txt', '-outputFormat', 'inlineXML', '2>>', 'error.log']\n",
      "['java', '-mx500m', '-classpath', 'stanford-ner.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', './classifiers/english.conll.4class.distsim.crf.ser.gz', '-textFile', './in/0_file.txt', '-outputFormat', 'inlineXML', '2>>', 'error.log']\n",
      "['java', '-mx500m', '-classpath', 'stanford-ner.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', './classifiers/english.muc.7class.distsim.crf.ser.gz', '-textFile', './in/0_file.txt', '-outputFormat', 'inlineXML', '2>>', 'error.log']\n",
      "['out-3class0_file.txt', 'out-4class0_file.txt', 'out-7class0_file.txt']\n",
      "Cleaning and Adding Columns...\n",
      "Writing Output...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    sys.path.append('/project/Development')\n",
    "    from helpers import general_helpers\n",
    "    import configparser\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('/project/Development/config.ini')\n",
    "\n",
    "    persistent_files = config['FOLDERS']['PERSISTENT_FILES']\n",
    "\n",
    "    pre_manual = '{}/government_interest/pre_manual'.format(config['FOLDERS']['WORKING_FOLDER'])\n",
    "    working_folder = config['FOLDERS']['WORKING_FOLDER']\n",
    "\n",
    "    # Set up vars + directories\n",
    "    merged_csv = '{}/merged_csvs.csv'.format(pre_manual)\n",
    "    ner_dir = \"/project/Development/government_interest/stanford-ner-2017-06-09/\"\n",
    "    ner_txt_indir = \"/project/Development/government_interest/stanford-ner-2017-06-09/in/\"\n",
    "    ner_txt_outdir = '{}/NER_out/'.format(pre_manual)\n",
    "    if not os.path.exists(ner_txt_outdir):\n",
    "        os.makedirs(ner_txt_outdir)\n",
    "    classifiers = ['classifiers/english.all.3class.distsim.crf.ser.gz', 'classifiers/english.conll.4class.distsim.crf.ser.gz', 'classifiers/english.muc.7class.distsim.crf.ser.gz']\n",
    "    ner_classif_dirs = ['out-3class', 'out-4class', 'out-7class']\n",
    "\n",
    "    final_output_dir = pre_manual\n",
    "\n",
    "    # 1. Merge csvs together and read in the input file\n",
    "    merged_df =  prepare_input_files(working_folder, merged_csv)\n",
    "\n",
    "    # # 2. run NER\n",
    "    run_NER(ner_dir, ner_txt_indir, ner_txt_outdir,merged_df, classifiers, ner_classif_dirs)\n",
    "\n",
    "    # 3. process NER output\n",
    "    orgs_list = process_NER(ner_txt_outdir, merged_df)\n",
    "\n",
    "    # 4. add extracted organizations and contract numbers\n",
    "    df_final,orgs_final = add_cols(merged_df, orgs_list)\n",
    "\n",
    "    # 5. write output file\n",
    "    write_output(final_output_dir,df_final,orgs_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
